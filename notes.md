- Replicate [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)
	- Authors see that networks with smaller weights are sensitive to smaller perturbations. LL probably not isotropic, origin breaks symmetry.
- Replicate learning rate scaling law. Is this the paper? [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/pdf/2203.03466)
	- See how roughness changes dependent on width of network
- Compare different nonlinearities and see if some are obviously 'better than others
	- Follow progression of the field: tanh, sigmoid, ReLU, GeLU
	- Do some weird (obviously bad) ones as well: gaussian, sin
- Try much more architectures
- Idee von Kaj: Anstatt roughness, mean free pathlength anschauen. Für gegebene Schrittgröße: wie weit kann ich in Richtung x gehen, vor ich der loss größer wird.
	- Alternativ wenn man was ähnlivheres zu curvature messen möchte: mit loss + grad*dist anstatt nur mit loss vergleichen.
	- Nachteil: jedes sample gibt nur ein bit an information: collision oder keine.
	- trotzdem vielleicht interessant!
- Talk with Arturs:
	- Hypotheses on what causes Roughness:
		- Complicated decision boundaries/fitting a detailed function
		- Numerical precision causes the increase of roughness towards the smaller scales
			- Experiment: Reduce precision.
		- More parameters/wider layers, cause more roughness. Compare to Tensor Programs.
	- Check out previous work on Fourier analysis of networks.
		- https://bmild.github.io/fourfeat/
- Why does the LL look like a quadratic potential in the limit for large numbers?
	- Maybe criterion? try other criteria
- Talk with Sebastian Lehner:
	- Try:
		- Different initialisations
		- Trained points
		- Different criterion functions
	- call it 'characteristic scales'
	- Just do a whole week of literature survey
