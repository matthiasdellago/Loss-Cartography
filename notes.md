- Replicate [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)
	- Authors see that networks with smaller weights are sensitive to smaller perturbations. LL probably not isotropic, origin breaks symmetry.
- Replicate learing rate scaling law. Is this the paper? [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/pdf/2203.03466)
	- See how roughness changes dependent on width of network
- Try different nonlinearities
	- And weird ones, like sines
- Try much more architectures

