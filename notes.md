- Replicate [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)
	- Authors see that networks with smaller weights are sensitive to smaller perturbations. LL probably not isotropic, origin breaks symmetry.
	- Look at radial vs tangential directions!
- Look in direction of gradient decent/ascent.
- Try different nonlinearities
	- And weird ones, like sines
- Try much more architectures

